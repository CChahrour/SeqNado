from itertools import chain
from pathlib import Path
from typing import Any, List

from loguru import logger
from pydantic import BaseModel, Field

from seqnado import Assay, DataScalingTechnique, PeakCallingMethod, PileupMethod
from seqnado import QuantificationMethod
from seqnado.config import SeqnadoConfig
from seqnado.inputs import (
    BamCollection,
    BigWigCollection,
    CollectionLike,
    FastqCollection,
    FastqCollectionForIP,
    SampleGroupings,
)
from seqnado.outputs.files import (
    BasicFileCollection,
    BigBedFiles,
    BigWigFiles,
    ContactFiles,
    CRISPRFiles,
    FileCollection,
    GeoSubmissionFiles,
    HeatmapFiles,
    HubFiles,
    MethylationFiles,
    PairFiles,
    PeakCallingFiles,
    PlotFiles,
    QCFiles,
    QuantificationFiles,
    SeqNadoReportFile,
    SNPFilesAnnotated,
    SNPFilesRaw,
    SpikeInFiles,
)

from seqnado.utils import FileSelector


class GeoMetadataFilesWrapper:
    """Wrapper that exposes only metadata files from GeoSubmissionFiles for Snakemake output tracking.

    This wrapper ensures that only the metadata files (checksums, samples table) that are
    declared as explicit rule outputs are included in the Snakemake OUTPUT object.
    The individual symlinked FASTQ and processed files are created by the geo_symlink rule
    but are not tracked as explicit outputs since they cannot be declared dynamically.
    """

    def __init__(self, geo_files: GeoSubmissionFiles):
        self._geo_files = geo_files

    @property
    def files(self) -> List[str]:
        """Return only metadata files."""
        return self._geo_files.metadata_files if self._geo_files.metadata_files else []

    def __bool__(self) -> bool:
        """Return True if there are any metadata files."""
        return bool(self.files)


class SeqnadoOutputFiles(BaseModel):
    """Collection of output files generated by SeqNado."""

    files: list[str] = Field(default_factory=list)
    sample_names: List[str] = Field(default_factory=list)
    ip_sample_names: List[str] = Field(default_factory=list)
    sample_groups: SampleGroupings | None = None
    assay: Assay | None = None
    config: Any = None
    design_dataframe: Any = None  # pd.DataFrame - use Any to avoid circular imports
    output_dir: str = "seqnado_output"

    class Config:
        arbitrary_types_allowed = True

    @property
    def all_files(self) -> List[str]:
        """Return all files in the output collection."""
        return self.files

    def select_files(
        self,
        suffix: str,
        include: str | None = None,
        exclude: str | None = None,
        must_include_all_patterns: bool = False,
        use_regex: bool = False,
        case_sensitive: bool = False,
    ) -> List[str]:
        """Filter files by suffix and optional substring.

        Args:
            suffix (str): The file suffix to filter by (e.g. ".txt" or "csv").
            include (str, optional): Substring or regex pattern that must be present in the file path.
            exclude (str, optional): Substring or regex pattern that must NOT be present in the file path.
            must_include_all_patterns (bool): If True, all include patterns must match (AND). If False, any match suffices (OR).
            use_regex (bool): If True, treat include/exclude as regex patterns.
            case_sensitive (bool): If True, matching is case-sensitive.
        Returns:
            List[str]: A list of file paths matching the criteria.
        """
        fs = FileSelector(self.files)
        return fs.select(
            suffix=suffix,
            includes=include,
            excludes=exclude,
            case_sensitive=case_sensitive,
            use_regex=use_regex,
            includes_all=must_include_all_patterns,
        )

    @property
    def bigwig_files(self):
        return self.select_files(".bigwig", exclude="/geo_submission/")

    def select_bigwig_subtype(
        self,
        method: PileupMethod = PileupMethod.DEEPTOOLS,
        scale: DataScalingTechnique = DataScalingTechnique.UNSCALED,
        spikein_method: str | None = None,
        assay: Assay | None = None,
        is_merged: bool = False,
        ip_only: bool = False,
    ):
        """Select bigWig files of a specific subtype.

        Args:
            method (PileupMethod): The pileup method to filter by.
            scale (DataScalingTechnique): The scale method to filter by.
            spikein_method (str, optional): The spike-in method to filter by orlando or with_input. Defaults to None. 
            assay (Assay, optional): The assay type to filter by. Defaults to None.
            is_merged (bool): If True, select merged (consensus) bigWigs only.
            ip_only (bool): If True, exclude control/input samples (requires ip_sample_names to be set).

        Returns:
            List[str]: A list of bigWig files matching the specified subtype.
        """
        includes = [method.value, scale.value]
        if is_merged:
            includes.append("merged")
        if spikein_method is not None:
            includes.append(spikein_method)
        if assay is not None:
            includes.append(assay.value.lower())

        results = self.select_files(
            ".bigWig",
            include=includes,
            exclude="/geo_submission/",
            must_include_all_patterns=True,
            case_sensitive=False,
        )
        # Separate individual from merged: both paths contain the scale string,
        # so filter explicitly on the presence/absence of "/merged/".
        if is_merged:
            results = [f for f in results if "/merged/" in f]
        else:
            results = [f for f in results if "/merged/" not in f]

        # Optionally filter to IP samples only (exclude control/input samples)
        if ip_only and self.ip_sample_names:
            ip_set = set(self.ip_sample_names)
            results = [f for f in results if Path(f).stem in ip_set]

        return results

    @property
    def peak_files(self):
        return self.select_files(".bed", exclude="/geo_submission/")

    @property
    def bigbed_files(self):
        return self.select_files(".bigBed")

    @property
    def has_consensus_peaks(self):
        """Check if consensus peaks are present in the output files."""
        return any(
            f.endswith(".bed") and DataScalingTechnique.MERGED.value in f
            for f in self.files
        )

    def _heatmap_dir(
        self,
        scale: DataScalingTechnique,
        method: PileupMethod = PileupMethod.DEEPTOOLS,
        is_merged: bool = False,
        spikein_method: str | None = None,
    ) -> str:
        """Return the heatmap output directory.

        Always returns a valid path string even if not yet registered, so that
        Snakemake rule definitions remain valid at parse time.
        When scale is SPIKEIN and spikein_method is provided, the path includes
        the spikein method as a subdirectory (e.g. spikein/orlando/).
        """
        prefix = "merged/" if is_merged else ""
        if scale == DataScalingTechnique.SPIKEIN and spikein_method is not None:
            scale_path = f"spikein/{spikein_method}"
        else:
            scale_path = scale.value
        return f"{self.output_dir}/heatmap/{prefix}{method.value}/{scale_path}"

    def select_heatmap_matrix(
        self,
        scale: DataScalingTechnique,
        method: PileupMethod = PileupMethod.DEEPTOOLS,
        is_merged: bool = False,
        spikein_method: str | None = None,
    ) -> str:
        """Return the matrix path (temp file, not in self.files)."""
        return f"{self._heatmap_dir(scale, method, is_merged, spikein_method)}/matrix.mat.gz"

    def select_heatmap_plot(
        self,
        scale: DataScalingTechnique,
        method: PileupMethod = PileupMethod.DEEPTOOLS,
        is_merged: bool = False,
        spikein_method: str | None = None,
    ) -> str:
        return f"{self._heatmap_dir(scale, method, is_merged, spikein_method)}/heatmap.pdf"

    def select_heatmap_metaplot(
        self,
        scale: DataScalingTechnique,
        method: PileupMethod = PileupMethod.DEEPTOOLS,
        is_merged: bool = False,
        spikein_method: str | None = None,
    ) -> str:
        return f"{self._heatmap_dir(scale, method, is_merged, spikein_method)}/metaplot.pdf"

    def select_genome_browser_plots(
        self,
        scale: DataScalingTechnique,
        method: PileupMethod = PileupMethod.DEEPTOOLS,
        is_merged: bool = False,
        spikein_method: str | None = None,
    ) -> list[str]:
        """Select genome browser plot files for a specific method/scale/merged combination.

        When scale is SPIKEIN and spikein_method is provided, only plots for
        that specific spike-in method are returned.
        """
        prefix = "merged/" if is_merged else ""
        if scale == DataScalingTechnique.SPIKEIN and spikein_method is not None:
            target = f"genome_browser_plots/{prefix}{method.value}/spikein/{spikein_method}/"
        else:
            target = f"genome_browser_plots/{prefix}{method.value}/{scale.value}/"
        return [f for f in self.files if target in f]

    @property
    def heatmap_files(self):
        return self.select_files(".pdf", include="heatmap")

    @property
    def genome_browser_plots(self):
        return [f for f in self.files if "genome_browser_plots" in f]

    @property
    def sentinel_files(self):
        return self.select_files(".txt", include=".mcc_")

    @property
    def ucsc_hub_files(self):
        return self.select_files(".txt", include="hub")


class SeqNadoReportFiles:
    def __init__(
        self,
        assay: Assay,
        samples: CollectionLike,
        config: SeqnadoConfig,
        sample_groupings: SampleGroupings = None,
        output_dir: str = "seqnado_output",
    ):
        """Initializes the SeqNadoReportFiles with the given assay, samples, config, sample_groupings, and output directory.
        Args:
            assay (Assay): The type of assay being processed.
            samples (SampleCollection | IPSampleCollection): The collection of samples to process.
            config (SeqnadoConfig): The configuration for the SeqNado project.
            sample_groupings (SampleGroupings, optional): Sample groupings for the project.
            output_dir (str): The output directory for all files. Defaults to "seqnado_output".
        """
        self.assay = assay
        self.samples = samples
        self.config = config
        self.sample_groupings = sample_groupings
        self.output_dir = output_dir

    @property
    def gather_input_files(self) -> List[str]:
        """Gather input files for the SeqNado report, excluding hub and report files.

        Returns:
            List[str]: A list of input file paths for the report.
        """
        builder = SeqnadoOutputBuilder(
            assay=self.assay,
            samples=self.samples,
            config=self.config,
            sample_groupings=self.sample_groupings,
            output_dir=self.output_dir,
        )
        # Add all file types except hub and report
        builder.add_qc_files()

        if (
            "create_bigwigs" in self.config.assay_config
            and self.config.assay_config.create_bigwigs
        ):
            builder.add_individual_bigwig_files()

        if (
            "call_peaks" in self.config.assay_config
            and self.config.assay_config.call_peaks
        ):
            builder.add_peak_files()

            # Add motif files for assays with peak calling
            peak_config = self.config.assay_config.peak_calling
            if (
                peak_config
                and peak_config.run_motif_analysis
                and peak_config.motif_method
            ):
                from seqnado import MotifMethod

                run_homer = MotifMethod.HOMER in peak_config.motif_method
                run_meme = MotifMethod.MEME in peak_config.motif_method
                builder.add_motif_files(
                    run_homer=run_homer,
                    run_meme=run_meme,
                )

        # Add quantification files for RNA assays with quantification method or if explicitly requested
        if (
            self.assay == Assay.RNA
            and hasattr(self.config.assay_config, "rna_quantification")
            and self.config.assay_config.rna_quantification
        ):
            builder.add_quantification_files()
        elif (
            "create_quantification_files" in self.config.assay_config
            and self.config.assay_config.create_quantification_files
        ):
            builder.add_quantification_files()

        if (
            "has_spikein" in self.config.assay_config
            and self.config.assay_config.has_spikein
        ):
            builder.add_spikein_files()

        if self.assay == Assay.SNP and self.config.assay_config.call_snps:
            builder.add_snp_files()

        if self.assay == Assay.METH and self.config.assay_config.call_methylation:
            builder.add_methylation_files()

        all_files = builder.build().all_files
        filtered_files = [
            f
            for f in all_files
            if not f.endswith(".hub.txt") and not f.endswith("seqnado_report.html")
        ]
        return filtered_files


class SeqnadoOutputBuilder:
    def __init__(
        self,
        assay: Assay,
        samples: CollectionLike,
        config: SeqnadoConfig,
        sample_groupings: SampleGroupings | None = None,
        output_dir: str = "seqnado_output",
    ):
        """Initializes the SeqnadoOutputBuilder with the given assay, samples, and configuration.
        Args:
            assay (Assay): The type of assay being processed.
            samples (SampleCollection | IPSampleCollection): The collection of samples to process.
            config (SeqnadoConfig): The configuration for the SeqNado project.
            sample_groupings (Optional[SampleGroupings]): Optional groups of samples for merging.
                If provided, will be used to create grouped bigwig files and grouped peak files.
            output_dir (str): The output directory for all files. Defaults to "seqnado_output".
        Raises:
            ValueError: If the provided assay is not supported or if sample groups are provided
                but not defined in the configuration.
        """

        self.assay = assay
        self.samples = samples
        self.config = config
        self.sample_groupings = sample_groupings
        self.output_dir = output_dir

        # Set sample groupings that do provide file names.
        # These need to be used when creating grouped bigwig and peak files.
        self.provide_filenames = ["consensus"]

        # Determine scale methods from config, default to UNSCALED if not specified
        # Default to UNSCALED when scale_methods aren't provided in config
        scale_methods_config = getattr(
            getattr(self.config.assay_config, "bigwigs", object()),
            "scale_methods",
            None,
        )

        # Convert string values to DataScalingTechnique enums if needed
        if scale_methods_config:
            self.scale_methods = [
                DataScalingTechnique(m) if isinstance(m, str) else m
                for m in scale_methods_config
            ]
        else:
            self.scale_methods = [DataScalingTechnique.UNSCALED]

        # Initialize an empty list to hold file collections
        self.file_collections: list[FileCollection] = []

    def add_qc_files(self) -> None:
        """Add quality control files to the output collection."""

        qc_files = QCFiles(
            assay=self.assay,
            samples=self.samples,
            output_dir=self.output_dir,
            config=self.config.qc,
        )
        self.file_collections.append(qc_files)

    def add_report_files(self) -> None:
        """Add report files to the output collection."""

        report_file = SeqNadoReportFile(
            output_dir=self.output_dir,
        )
        self.file_collections.append(report_file)

    def add_individual_bigwig_files(self) -> None:
        """Add individual bigwig files to the output collection."""
        from seqnado.inputs import FastqCollectionForIP

        unscaled_scales = [m for m in self.scale_methods if m == DataScalingTechnique.UNSCALED]
        normalized_scales = [m for m in self.scale_methods if m != DataScalingTechnique.UNSCALED]

        # Unscaled bigwigs are generated for all samples (including controls)
        if unscaled_scales:
            bigwig_files = BigWigFiles(
                assay=self.assay,
                names=self.samples.sample_names,
                pileup_methods=self.config.assay_config.bigwigs.pileup_method,
                scale_methods=unscaled_scales,
                output_dir=self.output_dir,
            )
            self.file_collections.append(bigwig_files)

        # Normalized (e.g. CSAW) bigwigs only apply to IP samples, not controls
        if normalized_scales:
            if isinstance(self.samples, FastqCollectionForIP):
                names = self.samples.ip_sample_names
            else:
                names = self.samples.sample_names
            bigwig_files = BigWigFiles(
                assay=self.assay,
                names=names,
                pileup_methods=self.config.assay_config.bigwigs.pileup_method,
                scale_methods=normalized_scales,
                output_dir=self.output_dir,
            )
            self.file_collections.append(bigwig_files)

    def add_spikein_bigwig_files(self) -> None:
        """Add spike-in normalized bigwig files to the output collection."""
        spikein_config = getattr(self.config.assay_config, 'spikein', None)
        spikein_methods = spikein_config.method if spikein_config else []

        bigwig_files = BigWigFiles(
            assay=self.assay,
            names=self.samples.sample_names,
            pileup_methods=self.config.assay_config.bigwigs.pileup_method,
            scale_methods=[DataScalingTechnique.SPIKEIN],
            spikein_methods=spikein_methods,
            output_dir=self.output_dir,
        )

        self.file_collections.append(bigwig_files)

    def add_grouped_bigwig_files(self) -> None:
        """Add grouped bigwig files to the output collection."""

        # Go through the sample groupings e.g. ['consensus', 'scaling']
        for group_name, sample_groups in self.sample_groupings.groupings.items():
            # If the group name is in the list of groups to provide filenames for
            if group_name not in self.provide_filenames:
                continue

            # If we are providing filenames for this group, create bigwig files for each group
            for group in sample_groups.groups:
                bigwig_files = BigWigFiles(
                    assay=self.assay,
                    names=[group.name],
                    pileup_methods=self.config.assay_config.bigwigs.pileup_method,
                    scale_methods=[DataScalingTechnique.UNSCALED],
                    output_dir=self.output_dir,
                    is_merged=True,
                )
                self.file_collections.append(bigwig_files)

    def add_grouped_normalized_bigwig_files(self) -> None:
        """Add CSAW-scaled merged bigwig files to the output collection."""

        normalized_scales = [
            s for s in self.scale_methods
            if s == DataScalingTechnique.CSAW
        ]
        if not normalized_scales:
            return

        consensus_groups = self.sample_groupings.groupings.get("consensus")
        if not consensus_groups:
            return

        for group in consensus_groups.groups:
            bigwig_files = BigWigFiles(
                assay=self.assay,
                names=[group.name],
                pileup_methods=self.config.assay_config.bigwigs.pileup_method,
                scale_methods=normalized_scales,
                output_dir=self.output_dir,
                is_merged=True,
            )
            self.file_collections.append(bigwig_files)

    def add_grouped_spikein_bigwig_files(self) -> None:
        """Add spike-in normalized merged bigwig files to the output collection."""

        spikein_config = getattr(self.config.assay_config, "spikein", None)
        spikein_methods = spikein_config.method if spikein_config else []
        if not spikein_methods:
            return

        consensus_groups = self.sample_groupings.groupings.get("consensus")
        if not consensus_groups:
            return

        for group in consensus_groups.groups:
            bigwig_files = BigWigFiles(
                assay=self.assay,
                names=[group.name],
                pileup_methods=self.config.assay_config.bigwigs.pileup_method,
                scale_methods=[DataScalingTechnique.SPIKEIN],
                spikein_methods=spikein_methods,
                output_dir=self.output_dir,
                is_merged=True,
            )
            self.file_collections.append(bigwig_files)

    def add_mcc_sentinel_pileup_files(self) -> None:
        """Add MCC sentinel files to the output collection.
        The issue with MCC bigwig files is that they are generated per viewpoint group.
        It's possible that we don't actually have the viewpoint coming through from the sample,
        this would cause a crash and a pipeline failure. So instead we just create a sentinel file
        to indicate that the bigwigs have been generated.
        """
        bigwigs = [
            Path(self.output_dir) / "bigwigs/mcc/.mcc_bigwigs_generated.txt",
        ]
        sentinel_files = BasicFileCollection(files=[str(bw) for bw in bigwigs])
        self.file_collections.append(sentinel_files)

    def add_mcc_sentinel_peak_files(self) -> None:
        """Add MCC sentinel files to the output collection.

        The issue with MCC peak files is that they are generated per viewpoint group.
        It's possible that we don't actually have the viewpoint coming through from the sample,
        this would cause a crash and a pipeline failure. So instead we just create a sentinel file
        to indicate that the peaks have been called.
        """
        peaks = [
            Path(self.output_dir) / "peaks/mcc/.mcc_peaks_called.txt",
        ]
        sentinel_files = BasicFileCollection(files=[str(pk) for pk in peaks])
        self.file_collections.append(sentinel_files)
    
    def add_mcc_sentinel_contact_files(self) -> None:
        """Add MCC sentinel files to the output collection.

        The issue with MCC contact files is that they are generated per viewpoint group.
        It's possible that we don't actually have the viewpoint coming through from the sample,
        this would cause a crash and a pipeline failure. So instead we just create a sentinel file
        to indicate that the contacts have been identified.
        """
        contacts = [
            Path(self.output_dir) / "mcc/.mcc_contacts_identified.txt",
        ]
        sentinel_files = BasicFileCollection(files=[str(ct) for ct in contacts])
        self.file_collections.append(sentinel_files)

    def add_peak_files(self) -> None:
        """Add peak files to the output collection."""

        # If starting from bigwigs, only lanceotron is allowed any other raises an error
        if isinstance(self.samples, BigWigCollection):
            if (
                self.config.assay_config.peak_calling
                and self.config.assay_config.peak_calling.method
                != [PeakCallingMethod.LANCEOTRON]
            ):
                raise ValueError(
                    f"For BigWigCollection, only {PeakCallingMethod.LANCEOTRON} is allowed."
                )

        # For IP-based assays, only call peaks on IP samples (not controls)
        from seqnado.inputs import FastqCollectionForIP

        if isinstance(self.samples, FastqCollectionForIP):
            sample_names = self.samples.ip_sample_names
        else:
            sample_names = self.samples.sample_names

        peaks = PeakCallingFiles(
            assay=self.assay,
            names=sample_names,
            peak_calling_method=self.config.assay_config.peak_calling.method,
            output_dir=self.output_dir,
        )
        self.file_collections.append(peaks)

    def add_motif_files(self, run_homer: bool = True, run_meme: bool = False) -> None:
        """Add motif analysis files to the output collection."""
        from seqnado.outputs.files import MotifFiles
        from seqnado.inputs import FastqCollectionForIP

        # For IP-based assays, only analyze motifs from IP samples
        if isinstance(self.samples, FastqCollectionForIP):
            sample_names = self.samples.ip_sample_names
        else:
            sample_names = self.samples.sample_names

        motifs = MotifFiles(
            assay=self.assay,
            names=sample_names,
            peak_calling_method=self.config.assay_config.peak_calling.method,
            output_dir=self.output_dir,
            run_homer=run_homer,
            run_meme=run_meme,
        )
        self.file_collections.append(motifs)

    def add_grouped_peak_files(self) -> None:
        """Add grouped peak files to the output collection."""

        # Go through the sample groupings e.g. ['consensus', 'scaling']
        if not self.sample_groupings:
            raise ValueError(
                "Sample groupings must be provided to add grouped peak files."
            )
        if "consensus" not in self.sample_groupings.groupings:
            raise ValueError(
                "Consensus groupings must be defined to add grouped peak files."
            )

        # Create peak files for each consensus group
        for group in self.sample_groupings.groupings["consensus"].groups:
            peaks = PeakCallingFiles(
                assay=self.assay,
                names=[group.name],
                peak_calling_method=self.config.assay_config.peak_calling.method,
                output_dir=self.output_dir,
                is_merged=True,
            )
            self.file_collections.append(peaks)

    def add_bigbed_files(self) -> None:
        """Add bigBed files to the output collection."""

        # Note this will build the bigBed files from the peak files already added
        # So ensure peak files are added before calling this method
        outfiles = self.build().peak_files
        bigbed_files = BigBedFiles(bed_files=outfiles)
        self.file_collections.append(bigbed_files)

    def add_heatmap_files(self) -> None:
        """Add heatmap files to the output collection.

        Creates one HeatmapFiles entry per (method, is_merged) combination,
        filtering out scale/method pairs that are incompatible or have no backing
        bigwigs (e.g. homer/bamnado individual bigwigs only support UNSCALED, and
        only when UNSCALED is actually in the configured scale methods).

        When spike-in normalization is configured, separate heatmap files are
        registered per spike-in method so each gets its own output directory.
        """
        has_spikein = getattr(self.config.assay_config, "has_spikein", False)
        has_consensus = bool(
            self.sample_groupings and self.sample_groupings.groupings.get("consensus")
        )
        bigwigs_config = self.config.assay_config.bigwigs
        if bigwigs_config is None or not bigwigs_config.pileup_method:
            pileup_methods = [PileupMethod.DEEPTOOLS]
        else:
            pileup_methods = bigwigs_config.pileup_method
        spikein_config = getattr(self.config.assay_config, "spikein", None)
        spikein_methods = spikein_config.method if spikein_config else []

        # Methods that only support UNSCALED for individual (not merged) bigwigs
        _unscaled_only_individual = {PileupMethod.HOMER, PileupMethod.BAMNADO}
        # Methods that only support UNSCALED for merged bigwigs
        _unscaled_only_merged = {PileupMethod.HOMER}

        for method in pileup_methods:
            for is_merged in ([False, True] if has_consensus else [False]):
                if is_merged:
                    restricted = method in _unscaled_only_merged
                else:
                    restricted = method in _unscaled_only_individual

                if restricted:
                    # Restricted methods only support UNSCALED.
                    # For individual bigwigs, unscaled files are only generated when
                    # UNSCALED is explicitly in scale_methods; skip if not present.
                    if not is_merged and DataScalingTechnique.UNSCALED not in self.scale_methods:
                        continue
                    scales = [DataScalingTechnique.UNSCALED]
                else:
                    scales = list(self.scale_methods)
                    if has_spikein and DataScalingTechnique.SPIKEIN not in scales:
                        scales.append(DataScalingTechnique.SPIKEIN)

                heatmaps = HeatmapFiles(
                    assay=self.assay,
                    scale_methods=scales,
                    spikein_methods=spikein_methods,
                    output_dir=self.output_dir,
                    is_merged=is_merged,
                    method=method,
                )
                self.file_collections.append(heatmaps)

    def add_hub_files(self) -> None:
        """Add hub files to the output collection."""
        hub_files = HubFiles(
            hub_dir=Path(f"{self.output_dir}/hub"),
            hub_name=getattr(self.config.assay_config.ucsc_hub, "name", "SeqnadoHub"),
        )
        self.file_collections.append(hub_files)

    def add_spikein_files(self) -> None:
        """Add spike-in files to the output collection."""
        spikein_config = getattr(self.config.assay_config, 'spikein', None)
        methods = spikein_config.method if spikein_config else []

        spikein_files = SpikeInFiles(
            assay=self.assay,
            names=self.samples.sample_names,
            method=methods,
            output_dir=self.output_dir,
        )
        self.file_collections.append(spikein_files)

    def add_plot_files(self) -> None:
        """Add plot files to the output collection.

        Creates one PlotFiles entry per (method, scale, is_merged) combination,
        applying the same incompatibility and availability filtering as
        add_heatmap_files().

        When spike-in normalization is configured, separate PlotFiles entries are
        created per spike-in method so each gets its own output directory.
        """
        has_spikein = getattr(self.config.assay_config, "has_spikein", False)
        has_consensus = bool(
            self.sample_groupings and self.sample_groupings.groupings.get("consensus")
        )
        bigwigs_config = self.config.assay_config.bigwigs
        if bigwigs_config is None or not bigwigs_config.pileup_method:
            pileup_methods = [PileupMethod.DEEPTOOLS]
        else:
            pileup_methods = bigwigs_config.pileup_method
        spikein_config = getattr(self.config.assay_config, "spikein", None)
        spikein_methods = spikein_config.method if spikein_config else []

        _unscaled_only_individual = {PileupMethod.HOMER, PileupMethod.BAMNADO}
        _unscaled_only_merged = {PileupMethod.HOMER}

        for method in pileup_methods:
            for is_merged in ([False, True] if has_consensus else [False]):
                if is_merged:
                    restricted = method in _unscaled_only_merged
                else:
                    restricted = method in _unscaled_only_individual

                if restricted:
                    if not is_merged and DataScalingTechnique.UNSCALED not in self.scale_methods:
                        continue
                    method_scales = [DataScalingTechnique.UNSCALED]
                else:
                    method_scales = list(self.scale_methods)
                    if has_spikein and DataScalingTechnique.SPIKEIN not in method_scales:
                        method_scales.append(DataScalingTechnique.SPIKEIN)

                for scale in method_scales:
                    if scale == DataScalingTechnique.SPIKEIN and spikein_methods:
                        # Create one PlotFiles per spike-in method for separate output dirs
                        for sm in spikein_methods:
                            plot_files = PlotFiles(
                                coordinates=self.config.assay_config.plotting.coordinates,
                                file_format=self.config.assay_config.plotting.file_format,
                                output_dir=self.output_dir,
                                scale=scale.value,
                                is_merged=is_merged,
                                method=method.value,
                                spikein_method=sm.value,
                            )
                            self.file_collections.append(plot_files)
                    else:
                        plot_files = PlotFiles(
                            coordinates=self.config.assay_config.plotting.coordinates,
                            file_format=self.config.assay_config.plotting.file_format,
                            output_dir=self.output_dir,
                            scale=scale.value,
                            is_merged=is_merged,
                            method=method.value,
                        )
                        self.file_collections.append(plot_files)

    def add_snp_files(self) -> None:
        """Add SNP files to the output collection."""
        snp_files_raw = SNPFilesRaw(
            assay=self.assay,
            names=self.samples.sample_names,
            output_dir=self.output_dir,
        )
        self.file_collections.append(snp_files_raw)

        if self.config.assay_config.snp_calling.annotate_snps:
            snp_files_annotated = SNPFilesAnnotated(
                assay=self.assay,
                names=self.samples.sample_names,
                output_dir=self.output_dir,
            )
            self.file_collections.append(snp_files_annotated)

    def add_methylation_files(self) -> None:
        """Add methylation files to the output collection."""
        methylation_files = MethylationFiles(
            assay=self.assay,
            names=self.samples.sample_names,
            genomes=self.config.assay_config.methylation.spikein_genomes,
            method=self.config.assay_config.methylation.method,
            output_dir=self.output_dir,
        )
        self.file_collections.append(methylation_files)

    def add_contact_files(self) -> None:
        """Add contact files to the output collection."""

        contact_files = ContactFiles(
            assay=self.assay,
            names=self.samples.sample_names
            if not self.sample_groupings
            else self.sample_groupings.get_grouping("consensus").group_names,
            output_dir=f"{self.output_dir}/mcc",
        )
        self.file_collections.append(contact_files)

    def add_crispr_files(self) -> None:
        """Add CRISPR-specific files (including MAGeCK outputs) to the output collection."""
        use_mageck = False
        if hasattr(self.config.assay_config, "use_mageck"):
            use_mageck = self.config.assay_config.use_mageck

        crispr_files = CRISPRFiles(
            use_mageck=use_mageck,
            output_dir=self.output_dir,
        )
        self.file_collections.append(crispr_files)

    def add_quantification_files(self) -> None:
        """Add quantification files to the output collection."""
        # Get the consensus grouping if it exists, otherwise use empty SampleGroups
        if self.sample_groupings and "consensus" in self.sample_groupings.groupings:
            groups = self.sample_groupings.groupings["consensus"]
        else:
            from seqnado.inputs.grouping import SampleGroups

            groups = SampleGroups(groups=[])

        quantification_files = QuantificationFiles(
            assay=self.assay,
            methods=[self.config.assay_config.rna_quantification.method]
            if hasattr(self.config.assay_config, "rna_quantification")
            else [QuantificationMethod.FEATURE_COUNTS],
            names=self.samples.sample_names,
            groups=groups,
            output_dir=self.output_dir,
        )
        self.file_collections.append(quantification_files)

    def add_geo_submission_files(self) -> None:
        """Add files for GEO submission.

        **Note**: This method builds the output files collection
        and appends it to the file_collections list. So it should be called
        after all other file collections have been added if you want to include GEO files in the final output.

        Only the metadata files (checksums, samples table) are added to the output,
        not the individual symlinked FASTQ and processed files, as those cannot be
        declared as dynamic rule outputs in Snakemake.
        """

        if isinstance(self.samples, (BamCollection, BigWigCollection)):
            raise ValueError(
                "GEO submission files can only be generated from FASTQ inputs."
            )

        outfiles = self.build().all_files
        geo_files = GeoSubmissionFiles(
            assay=self.assay,
            names=self.samples.sample_names,
            seqnado_files=outfiles,
            output_dir=self.output_dir,
            samples=self.samples,  # Pass samples so we can check if paired-end
        )
        self.file_collections.append(GeoMetadataFilesWrapper(geo_files))

    def build(self) -> SeqnadoOutputFiles:
        """Builds the output files collection based on the added file collections."""
        all_files = list(
            chain.from_iterable(p.files for p in self.file_collections if p.files)
        )

        # Get design dataframe from samples if available
        design_df = None
        if hasattr(self.samples, "to_dataframe"):
            design_df = self.samples.to_dataframe()

        from seqnado.inputs import FastqCollectionForIP
        ip_sample_names = (
            self.samples.ip_sample_names
            if isinstance(self.samples, FastqCollectionForIP)
            else []
        )

        return SeqnadoOutputFiles(
            files=all_files,
            sample_names=self.samples.sample_names,
            ip_sample_names=ip_sample_names,
            sample_groups=self.sample_groupings,
            assay=self.assay,
            config=self.config,
            design_dataframe=design_df,
            output_dir=self.output_dir,
        )


class MultiomicsOutputBuilder:
    """Defines the output files for multiomics analysis."""

    def __init__(
        self,
        output_dir: Path | None = None,
        assay_outputs: dict[Assay, SeqnadoOutputFiles] | None = None,
    ):
        self.output_dir = output_dir or Path("seqnado_output")
        self.assay_outputs = assay_outputs or {}
        self.file_collections: list[FileCollection] = []

    def add_assay_bigwigs(self) -> list[str]:
        """Get all bigwigs from assay-specific 'all' rules."""
        bigwigs = []
        for assay, output_files in self.assay_outputs.items():
            bws = output_files.bigwig_files
            bigwigs.extend(bws)
        self.file_collections.append(BasicFileCollection(files=bigwigs))

    def add_assay_peaks(self) -> list[str]:
        """Get all peak files from assay-specific 'all' rules."""
        peaks = []
        for assay, output_files in self.assay_outputs.items():
            pks = output_files.peak_files
            peaks.extend(pks)
        self.file_collections.append(BasicFileCollection(files=peaks))

    def add_summary_report(self) -> str:
        """Path to the multiomics summary report."""
        path = str(Path(self.output_dir) / "multiomics_summary.txt")
        self.file_collections.append(BasicFileCollection(files=[path]))

    def add_heatmap(self) -> str:
        """Path to the multiomics heatmap PDF."""
        path = str(Path(self.output_dir) / "multiomics" / "heatmap" / "heatmap.pdf")
        self.file_collections.append(BasicFileCollection(files=[path]))

    def add_metaplot(self) -> str:
        """Path to the multiomics metaplot PDF."""
        path = str(Path(self.output_dir) / "multiomics" / "heatmap" / "metaplot.pdf")
        self.file_collections.append(BasicFileCollection(files=[path]))

    def add_multiomics_dataset(self, use_binsize: bool = True) -> str:
        """Add the multiomics dataset output file.

        Args:
            use_binsize: If True, adds dataset_bins.h5ad (binsize mode).
                        If False, adds dataset_regions.h5ad (regions mode).
        """
        filename = "dataset_bins.h5ad" if use_binsize else "dataset_regions.h5ad"
        path = str(
            Path(self.output_dir) / "multiomics" / "dataset" / filename
        )
        self.file_collections.append(BasicFileCollection(files=[path]))

    def add_assay_outputs(self) -> None:
        """Add all assay output files to the multiomics output collection."""
        for assay, output_files in self.assay_outputs.items():
            self.file_collections.append(
                BasicFileCollection(files=output_files.all_files)
            )

    def build(self) -> SeqnadoOutputFiles:
        """Builds the output files collection based on the added file collections."""
        all_files = list(
            chain.from_iterable(p.files for p in self.file_collections if p.files)
        )
        return SeqnadoOutputFiles(
            files=all_files,
            sample_names=[],
            sample_groups=None,
            assay=None,
            config=None,
            design_dataframe=None,
            output_dir=str(self.output_dir),
        )


class SeqnadoOutputFactory:
    def __init__(
        self,
        assay: Assay,
        samples: FastqCollection | FastqCollectionForIP,
        config: SeqnadoConfig,
        sample_groupings: SampleGroupings | None = None,
        output_dir: str = "seqnado_output",
    ):
        self.assay = assay
        self.samples = samples
        self.config = config
        self.sample_groupings = sample_groupings
        self.assay_config = config.assay_config
        self.output_dir = output_dir

    def create_output_builder(self) -> SeqnadoOutputBuilder:
        """Creates a SeqnadoOutputBuilder instance with the provided assay, samples, and configuration.

        Returns:
            SeqnadoOutputBuilder: An instance of SeqnadoOutputBuilder configured with the provided parameters.
        Raises:
            ValueError: If the provided assay is not supported or if sample groups are provided
                but not defined in the configuration.
        """
        builder = SeqnadoOutputBuilder(
            assay=self.assay,
            samples=self.samples,
            config=self.config,
            sample_groupings=self.sample_groupings,
            output_dir=self.output_dir,
        )

        # Only add QC files and reports for assays that support them (those with QC tools configured)
        qc_supported_assays = [
            Assay.ATAC,
            Assay.CHIP,
            Assay.CAT,
            Assay.RNA,
            Assay.SNP,
            Assay.METH,
            Assay.MCC,
            Assay.CRISPR,
        ]
        if self.assay in qc_supported_assays:
            builder.add_qc_files()
            builder.add_report_files()

        if self.assay_config.create_bigwigs:
            if not self.assay == Assay.MCC:
                builder.add_individual_bigwig_files()
                if self.sample_groupings:
                    builder.add_grouped_bigwig_files()
                    builder.add_grouped_normalized_bigwig_files()
            else:
                builder.add_mcc_sentinel_pileup_files()

        if bool(getattr(self.assay_config, "call_peaks", False)):
            if not self.assay == Assay.MCC:
                builder.add_peak_files()
                if self.sample_groupings.groupings.get("consensus"):
                    builder.add_grouped_peak_files()
            else:
                builder.add_mcc_sentinel_peak_files()

            # Add motif files if motif analysis is enabled
            peak_config = self.assay_config.peak_calling
            if (
                peak_config
                and peak_config.run_motif_analysis
                and peak_config.motif_method
            ):
                from seqnado import MotifMethod

                run_homer = MotifMethod.HOMER in peak_config.motif_method
                run_meme = MotifMethod.MEME in peak_config.motif_method
                builder.add_motif_files(
                    run_homer=run_homer,
                    run_meme=run_meme,
                )

        if self.assay_config.create_heatmaps:
            builder.add_heatmap_files()

        if self.assay_config.create_ucsc_hub:
            builder.add_hub_files()

        if self.assay_config.create_geo_submission_files:
            builder.add_geo_submission_files()

        if getattr(self.assay_config, "has_spikein", False):
            builder.add_spikein_files()
            if self.assay_config.create_bigwigs:
                builder.add_spikein_bigwig_files()
                if self.sample_groupings:
                    builder.add_grouped_spikein_bigwig_files()

        if getattr(self.assay_config, "create_quantification_files", False):
            builder.add_quantification_files()

        # Add additional files based on the assay type
        match self.assay:
            case Assay.ATAC | Assay.CHIP | Assay.CAT | Assay.RNA:
                if self.assay_config.plot_with_plotnado:
                    builder.add_plot_files()
            case Assay.CRISPR:
                builder.add_crispr_files()
            case Assay.SNP:
                if self.assay_config.call_snps:
                    builder.add_snp_files()
            case Assay.METH:
                if self.assay_config.call_methylation:
                    builder.add_methylation_files()
            case Assay.MCC:
                builder.add_mcc_sentinel_contact_files()
            case _:
                logger.debug(f"No additional files to add for assay {self.assay}")

        return builder
